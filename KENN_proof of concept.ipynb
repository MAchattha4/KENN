{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "seed_value= 1111\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "# from sklearn.metrics import mean_absolute_percentage_error\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input,LSTM, Dense, Flatten, Conv1D, Lambda, Reshape, RepeatVector\n",
    "from keras.layers.merge import concatenate, multiply,add\n",
    "from keras import regularizers\n",
    "from keras.initializers import glorot_uniform\n",
    "from tqdm import tqdm\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import random as python_random\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to make input window for training and test sets\n",
    "\n",
    "def make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]\n",
    "    y = np.zeros([length-window_size+1-horizon,horizon])\n",
    "    output=np.zeros([length-window_size+1-horizon,window_size])\n",
    "    for i in range(length-window_size-horizon+1):\n",
    "        output[i:i+1,:]=data[i:i+window_size]\n",
    "        y[i,:]= data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],window_size), y\n",
    "\n",
    "def make_k_input(data,horizon):\n",
    "    length = data.shape[0]\n",
    "    output= np.zeros([length+1-horizon,horizon])\n",
    "    for i in range(length-horizon+1):\n",
    "        output[i:i+1,:]=data[i:i+horizon]\n",
    "    return output.reshape(output.shape[0],horizon)\n",
    "\n",
    "def nonov_make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "\n",
    "    data = np.append(data,np.zeros([horizon-extra]))\n",
    "\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "        \n",
    "    output=np.zeros([i_val,window_size])\n",
    "    y=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[i*horizon:(i*horizon)+window_size]\n",
    "        y[i,:]= data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "        \n",
    "    return output.reshape(output.shape[0],window_size), y\n",
    "\n",
    "def nonov_make_k_input(data,horizon):\n",
    "    length = data.shape[0]\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "    data_app = np.repeat(data[-1],(horizon-extra))\n",
    "    data = np.append(data,data_app)    \n",
    "\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "    output=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[(i*horizon):(i*horizon)+horizon]\n",
    "    return output.reshape(output.shape[0],horizon)\n",
    "\n",
    "\n",
    "def metrics(pred,gt):\n",
    "    l = pred.shape[1]\n",
    "#     print(l)\n",
    "    err_mse = np.zeros((l))\n",
    "    err_mae = np.zeros((l))\n",
    "\n",
    "    for i in range(l):\n",
    "        err_mse[i] = mse(pred[:,i],gt[:,i])\n",
    "        err_mae[i] = mae(pred[:,i],gt[:,i])\n",
    "        \n",
    "    return np.sqrt(np.mean(err_mse)),np.mean(err_mae)\n",
    "\n",
    "def concat(data):\n",
    "    temp=np.zeros((data.shape[0]*207,data.shape[1]))\n",
    "    for i in range(207):\n",
    "        temp[i*data.shape[0]:(i+1)*data.shape[0],:]=data[:,:,i]\n",
    "    return temp\n",
    "\n",
    "def window_normalize(data_x,data_y):\n",
    "    \n",
    "    min_in = data_x.min(1).reshape(data_x.shape[0],1)\n",
    "    max_in = data_x.max(1).reshape(data_x.shape[0],1)\n",
    "    denom = (max_in-min_in)\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size >0:\n",
    "        denom[a]=1\n",
    "    out = (data_x-min_in)/denom\n",
    "    out=out.reshape(out.shape[0],out.shape[1],1)\n",
    "    out_y=(data_y-min_in)/denom\n",
    "    return out,out_y,denom,min_in\n",
    "\n",
    "def std_window_normalize(data_x,data_y):\n",
    "    \n",
    "    mean_in = np.mean(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    std_in = np.std(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    \n",
    "    a = np.where(std_in == 0)[0]\n",
    "    std_in[a] = 1\n",
    "    \n",
    "    out = (data_x-mean_in)/std_in\n",
    "    out=out.reshape(out.shape[0],out.shape[1],1)\n",
    "    out_y=(data_y-mean_in)/std_in\n",
    "    return out,out_y,std_in,mean_in\n",
    "\n",
    "def p_window_normalize(data_x):\n",
    "    \n",
    "    min_in = data_x.min(1).reshape(data_x.shape[0],1)\n",
    "    max_in = data_x.max(1).reshape(data_x.shape[0],1)\n",
    "    denom = (max_in-min_in)\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size >0:\n",
    "        denom[a]=1\n",
    "    out = (data_x-min_in)/denom\n",
    "    out=out.reshape(out.shape[0],out.shape[1])\n",
    "    \n",
    "    return out\n",
    "\n",
    "def p_std_window_normalize(data_x):\n",
    "    \n",
    "    mean_in = np.mean(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    std_in = np.std(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    \n",
    "    a = np.where(std_in == 0)[0]\n",
    "    std_in[a] = 1\n",
    "    \n",
    "    out = (data_x-mean_in)/std_in\n",
    " \n",
    "    out=out.reshape(out.shape[0],out.shape[1])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('/home/chatta/logic_rules/data/traffic_30_with_std.csv',usecols=['avg_flow','std'])\n",
    "std_data=np.asarray(data['std'])\n",
    "data= np.asarray(data['avg_flow'])\n",
    "data1=data[47:]\n",
    "horizon = 1\n",
    "window_size=3\n",
    "knowledge_preds=np.asarray(pd.read_csv(\"/home/chatta/logic_rules/results/pacf_preds/traffic_30_std/h1/preds_noisy.csv\",header=None))\n",
    "n_train=int(0.8*data.size)+1\n",
    "n_test=int(.1*data.size)\n",
    "n_val=2*n_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_d = data1[:-n_val]\n",
    "# n_train=train_d.size\n",
    "# n_reduced = int(0.1*n_train)\n",
    "# train_d=train_d[-(n_reduced+window_size):]\n",
    "val_d=data1[-(n_val+window_size):-n_test]\n",
    "test_d=data1[-(n_test+window_size):]\n",
    "\n",
    "current_pred= knowledge_preds\n",
    "train_p = current_pred[:-n_val]\n",
    "# train_p=train_p[-n_reduced:]\n",
    "val_p = current_pred[-n_val:-n_test]\n",
    "test_p = current_pred[-n_test:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1354"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--------------------------concept-----------------------------\n",
    "\n",
    "\n",
    "train_sequence = make_input(train_d, window_size,horizon)\n",
    "val_sequence = make_input(val_d,window_size,horizon)\n",
    "test_sequence = nonov_make_input(test_d,window_size,horizon)\n",
    "\n",
    "\n",
    "\n",
    "train_pred = make_k_input(train_p,horizon)\n",
    "val_pred = make_k_input(val_p,horizon)\n",
    "test_pred = nonov_make_k_input(test_p,horizon)\n",
    "\n",
    "norm_train_x,norm_train_y,denom_train,min_train=std_window_normalize(train_sequence[0],train_sequence[1])\n",
    "norm_val_x,norm_val_y,denom_val,min_val=std_window_normalize(val_sequence[0],val_sequence[1])  \n",
    "norm_test_x,norm_test_y,denom_test,min_test=std_window_normalize(test_sequence[0],test_sequence[1])\n",
    "\n",
    "norm_p_train_x=(train_pred-min_train)/denom_train \n",
    "norm_p_val_x=(val_pred-min_val)/denom_val\n",
    "norm_p_test_x=(test_pred-min_test)/denom_test \n",
    "\n",
    "train_input=np.repeat(norm_p_train_x,window_size,axis=1)\n",
    "val_input=np.repeat(norm_p_val_x,window_size,axis=1)\n",
    "test_input=np.repeat(norm_p_test_x,window_size,axis=1)\n",
    "\n",
    "train_input=train_input.reshape(train_input.shape[0],train_input.shape[1],1)\n",
    "val_input=val_input.reshape(val_input.shape[0],val_input.shape[1],1)\n",
    "test_input=test_input.reshape(test_input.shape[0],test_input.shape[1],1)\n",
    "\n",
    "final_in_train =np.append(norm_train_x,train_input,axis=2)\n",
    "\n",
    "\n",
    "final_in_val =np.append(norm_val_x,val_input,axis=2)\n",
    "\n",
    "\n",
    "final_in_test =np.append(norm_test_x,test_input,axis=2)\n",
    "\n",
    "final_lbl_train = norm_train_y\n",
    "final_lbl_val= norm_val_y\n",
    "final_lbl_test = norm_test_y\n",
    "\n",
    "final_p_train= norm_p_train_x\n",
    "final_p_test= norm_p_test_x\n",
    "final_p_val= norm_p_val_x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10785, 3, 1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "K.clear_session()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_data= Input(batch_shape=(None,window_size,2),name='input_data')\n",
    "input_pred=Input(batch_shape=(None,horizon),name='input_pred')\n",
    "\n",
    "\n",
    "# branch_0 = Conv1D(4,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform())(input_data)\n",
    "# branch_1 = Conv1D(32,3, strides=1,dilation_rate=1,padding='same',activation='relu',kernel_initializer=glorot_uniform())(branch_0)\n",
    "# branch_2 = Conv1D(64,3, strides=1,dilation_rate=1, padding='same',activation='relu',kernel_initializer=glorot_uniform())(branch_1)\n",
    "\n",
    "# branch_4=Flatten()(branch_2)\n",
    "# net= Dense(128,activation='relu')(branch_4)\n",
    "# net= Dense(horizon,activation='relu')(net)\n",
    "# net=add([net,input_pred])\n",
    "#         ,activity_regularizer=regularizers.l2(0.01)\n",
    "\n",
    "branch_0 = LSTM(512,activation='sigmoid',kernel_initializer=glorot_uniform(1),return_sequences=True,name='lstm_1')(input_data)\n",
    "branch_1 = LSTM(64,activation='relu',kernel_initializer=glorot_uniform(2),return_sequences=True,name='lstm_2')(branch_0)\n",
    "branch_2 = LSTM(16,activation='relu',kernel_initializer=glorot_uniform(3),name='lstm_3')(branch_1)\n",
    "net= Dense(horizon,name='dense_final2')(branch_2)\n",
    "# net=add([net,input_pred])\n",
    "\n",
    "# model=Model(inputs=[input_data,input_pred],outputs=net)\n",
    "model=Model(inputs=[input_data],outputs=net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10785 samples, validate on 1354 samples\n",
      "Epoch 1/300\n",
      "10785/10785 [==============================] - 3s 255us/step - loss: 60.9611 - val_loss: 207.2730\n",
      "Epoch 2/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 60.3084 - val_loss: 206.5359\n",
      "Epoch 3/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 59.6171 - val_loss: 205.6870\n",
      "Epoch 4/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 58.7923 - val_loss: 204.6285\n",
      "Epoch 5/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 57.7402 - val_loss: 203.2504\n",
      "Epoch 6/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 56.3563 - val_loss: 201.4150\n",
      "Epoch 7/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 54.5548 - val_loss: 199.2045\n",
      "Epoch 8/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 52.5999 - val_loss: 197.2423\n",
      "Epoch 9/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 51.3329 - val_loss: 196.3172\n",
      "Epoch 10/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 50.5625 - val_loss: 194.5608\n",
      "Epoch 11/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 48.8228 - val_loss: 192.2205\n",
      "Epoch 12/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 46.8144 - val_loss: 189.2475\n",
      "Epoch 13/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 44.5962 - val_loss: 186.0620\n",
      "Epoch 14/300\n",
      "10785/10785 [==============================] - 0s 18us/step - loss: 42.3746 - val_loss: 182.3648\n",
      "Epoch 15/300\n",
      "10785/10785 [==============================] - 0s 18us/step - loss: 39.8687 - val_loss: 178.0819\n",
      "Epoch 16/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 37.1496 - val_loss: 173.4626\n",
      "Epoch 17/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 34.3890 - val_loss: 168.8285\n",
      "Epoch 18/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 31.7772 - val_loss: 164.1582\n",
      "Epoch 19/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 29.2676 - val_loss: 159.2947\n",
      "Epoch 20/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 26.8727 - val_loss: 154.4641\n",
      "Epoch 21/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 24.7403 - val_loss: 149.8997\n",
      "Epoch 22/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 22.9267 - val_loss: 145.6313\n",
      "Epoch 23/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 21.4554 - val_loss: 141.5706\n",
      "Epoch 24/300\n",
      "10785/10785 [==============================] - 0s 18us/step - loss: 20.3166 - val_loss: 137.7995\n",
      "Epoch 25/300\n",
      "10785/10785 [==============================] - 0s 18us/step - loss: 19.5311 - val_loss: 134.4124\n",
      "Epoch 26/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 18.9182 - val_loss: 131.3119\n",
      "Epoch 27/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 18.1416 - val_loss: 128.4758\n",
      "Epoch 28/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 17.5660 - val_loss: 126.1882\n",
      "Epoch 29/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 17.0972 - val_loss: 124.0298\n",
      "Epoch 30/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 16.6196 - val_loss: 122.0562\n",
      "Epoch 31/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 16.2863 - val_loss: 120.0262\n",
      "Epoch 32/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 15.8579 - val_loss: 118.2527\n",
      "Epoch 33/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 15.6043 - val_loss: 116.4915\n",
      "Epoch 34/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 15.2084 - val_loss: 114.9104\n",
      "Epoch 35/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 14.9729 - val_loss: 113.2853\n",
      "Epoch 36/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 14.6722 - val_loss: 111.7052\n",
      "Epoch 37/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 14.4550 - val_loss: 110.1960\n",
      "Epoch 38/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 14.2370 - val_loss: 108.7819\n",
      "Epoch 39/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 14.0677 - val_loss: 107.4613\n",
      "Epoch 40/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 13.8942 - val_loss: 106.2616\n",
      "Epoch 41/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 13.7406 - val_loss: 105.1368\n",
      "Epoch 42/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 13.5982 - val_loss: 104.0192\n",
      "Epoch 43/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 13.4635 - val_loss: 102.9599\n",
      "Epoch 44/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 13.3368 - val_loss: 101.9756\n",
      "Epoch 45/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 13.2232 - val_loss: 101.0264\n",
      "Epoch 46/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 13.1107 - val_loss: 100.1365\n",
      "Epoch 47/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 13.0057 - val_loss: 99.2955\n",
      "Epoch 48/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 12.9040 - val_loss: 98.5202\n",
      "Epoch 49/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 12.8161 - val_loss: 97.7934\n",
      "Epoch 50/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 12.7324 - val_loss: 97.1032\n",
      "Epoch 51/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 12.6563 - val_loss: 96.4471\n",
      "Epoch 52/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 12.5853 - val_loss: 95.8516\n",
      "Epoch 53/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 12.5263 - val_loss: 95.3212\n",
      "Epoch 54/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 12.4673 - val_loss: 94.7890\n",
      "Epoch 55/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 12.4004 - val_loss: 94.2762\n",
      "Epoch 56/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 12.3406 - val_loss: 93.7931\n",
      "Epoch 57/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 12.2822 - val_loss: 93.3221\n",
      "Epoch 58/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 12.2038 - val_loss: 92.8331\n",
      "Epoch 59/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 12.0990 - val_loss: 92.8252\n",
      "Epoch 60/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 12.0808 - val_loss: 92.3361\n",
      "Epoch 61/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 12.0377 - val_loss: 91.4781\n",
      "Epoch 62/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.9643 - val_loss: 90.9074\n",
      "Epoch 63/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.8771 - val_loss: 90.6272\n",
      "Epoch 64/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.8180 - val_loss: 90.4789\n",
      "Epoch 65/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.7738 - val_loss: 89.7725\n",
      "Epoch 66/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.6910 - val_loss: 89.1037\n",
      "Epoch 67/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.6131 - val_loss: 88.7644\n",
      "Epoch 68/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.5482 - val_loss: 88.0928\n",
      "Epoch 69/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.4912 - val_loss: 87.5377\n",
      "Epoch 70/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.3988 - val_loss: 87.2960\n",
      "Epoch 71/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.3312 - val_loss: 86.6215\n",
      "Epoch 72/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.2722 - val_loss: 86.0808\n",
      "Epoch 73/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.1751 - val_loss: 85.8666\n",
      "Epoch 74/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.1245 - val_loss: 85.1654\n",
      "Epoch 75/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10785/10785 [==============================] - 0s 15us/step - loss: 11.0773 - val_loss: 84.6567\n",
      "Epoch 76/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 10.9849 - val_loss: 84.5256\n",
      "Epoch 77/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 10.9314 - val_loss: 83.8136\n",
      "Epoch 78/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 10.8537 - val_loss: 83.2275\n",
      "Epoch 79/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 10.7730 - val_loss: 82.5900\n",
      "Epoch 80/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 10.7355 - val_loss: 82.0911\n",
      "Epoch 81/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 10.6396 - val_loss: 81.6924\n",
      "Epoch 82/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 10.5748 - val_loss: 81.1485\n",
      "Epoch 83/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 10.5381 - val_loss: 80.6752\n",
      "Epoch 84/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 10.4494 - val_loss: 80.5106\n",
      "Epoch 85/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 10.4146 - val_loss: 79.7661\n",
      "Epoch 86/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 10.2764 - val_loss: 79.0820\n",
      "Epoch 87/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 10.2064 - val_loss: 78.6977\n",
      "Epoch 88/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 10.1576 - val_loss: 78.2313\n",
      "Epoch 89/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 10.1612 - val_loss: 77.7528\n",
      "Epoch 90/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 10.0841 - val_loss: 77.2900\n",
      "Epoch 91/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 10.0269 - val_loss: 76.8416\n",
      "Epoch 92/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.9944 - val_loss: 76.3367\n",
      "Epoch 93/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 9.8717 - val_loss: 75.8186\n",
      "Epoch 94/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.8635 - val_loss: 75.4551\n",
      "Epoch 95/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.7897 - val_loss: 74.9155\n",
      "Epoch 96/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.7780 - val_loss: 74.4009\n",
      "Epoch 97/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.6662 - val_loss: 73.8888\n",
      "Epoch 98/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.6629 - val_loss: 73.5663\n",
      "Epoch 99/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.6141 - val_loss: 73.0206\n",
      "Epoch 100/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.5886 - val_loss: 72.8185\n",
      "Epoch 101/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.4870 - val_loss: 72.0136\n",
      "Epoch 102/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.5237 - val_loss: 71.8447\n",
      "Epoch 103/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.4720 - val_loss: 71.1958\n",
      "Epoch 104/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 9.4289 - val_loss: 70.9323\n",
      "Epoch 105/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 9.3301 - val_loss: 70.2050\n",
      "Epoch 106/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.3269 - val_loss: 70.0235\n",
      "Epoch 107/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.3213 - val_loss: 69.4180\n",
      "Epoch 108/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 9.2171 - val_loss: 68.9719\n",
      "Epoch 109/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.1967 - val_loss: 68.6167\n",
      "Epoch 110/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.2136 - val_loss: 68.1552\n",
      "Epoch 111/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.1470 - val_loss: 67.8016\n",
      "Epoch 112/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.1574 - val_loss: 67.5206\n",
      "Epoch 113/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 9.1192 - val_loss: 67.1382\n",
      "Epoch 114/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.0831 - val_loss: 66.6145\n",
      "Epoch 115/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.0192 - val_loss: 66.1934\n",
      "Epoch 116/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.0381 - val_loss: 65.8972\n",
      "Epoch 117/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 9.0255 - val_loss: 65.4030\n",
      "Epoch 118/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 8.9310 - val_loss: 64.9025\n",
      "Epoch 119/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 8.9168 - val_loss: 64.5728\n",
      "Epoch 120/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.9143 - val_loss: 64.1176\n",
      "Epoch 121/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.8481 - val_loss: 63.6946\n",
      "Epoch 122/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.8387 - val_loss: 63.3501\n",
      "Epoch 123/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.8417 - val_loss: 63.0004\n",
      "Epoch 124/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.8581 - val_loss: 62.5902\n",
      "Epoch 125/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.7431 - val_loss: 62.1087\n",
      "Epoch 126/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.7384 - val_loss: 61.9523\n",
      "Epoch 127/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.7463 - val_loss: 61.6356\n",
      "Epoch 128/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.7782 - val_loss: 61.2747\n",
      "Epoch 129/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.6734 - val_loss: 60.8244\n",
      "Epoch 130/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.6563 - val_loss: 60.7006\n",
      "Epoch 131/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.6706 - val_loss: 60.3755\n",
      "Epoch 132/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.6658 - val_loss: 60.1845\n",
      "Epoch 133/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.6812 - val_loss: 59.8384\n",
      "Epoch 134/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 8.6014 - val_loss: 59.5981\n",
      "Epoch 135/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.5780 - val_loss: 59.4381\n",
      "Epoch 136/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.5323 - val_loss: 59.1134\n",
      "Epoch 137/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.5742 - val_loss: 58.9464\n",
      "Epoch 138/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.5701 - val_loss: 58.5545\n",
      "Epoch 139/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.5222 - val_loss: 58.3432\n",
      "Epoch 140/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.5110 - val_loss: 58.1742\n",
      "Epoch 141/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.4982 - val_loss: 57.9432\n",
      "Epoch 142/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.5328 - val_loss: 57.7259\n",
      "Epoch 143/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.4522 - val_loss: 57.3583\n",
      "Epoch 144/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.4447 - val_loss: 57.3136\n",
      "Epoch 145/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.4690 - val_loss: 57.0737\n",
      "Epoch 146/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.4915 - val_loss: 56.8298\n",
      "Epoch 147/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.4414 - val_loss: 56.4993\n",
      "Epoch 148/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.4250 - val_loss: 56.4026\n",
      "Epoch 149/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 8.4392 - val_loss: 56.1125\n",
      "Epoch 150/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.3760 - val_loss: 55.8061\n",
      "Epoch 151/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.3569 - val_loss: 55.6762\n",
      "Epoch 152/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.3372 - val_loss: 55.4700\n",
      "Epoch 153/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.3450 - val_loss: 55.1892\n",
      "Epoch 154/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.2431 - val_loss: 54.8854\n",
      "Epoch 155/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.2448 - val_loss: 54.7892\n",
      "Epoch 156/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.2690 - val_loss: 54.4760\n",
      "Epoch 157/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.2054 - val_loss: 54.2030\n",
      "Epoch 158/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.2149 - val_loss: 54.0763\n",
      "Epoch 159/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.2171 - val_loss: 53.7473\n",
      "Epoch 160/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.1856 - val_loss: 53.5299\n",
      "Epoch 161/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.1928 - val_loss: 53.3692\n",
      "Epoch 162/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.1917 - val_loss: 53.0574\n",
      "Epoch 163/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.1613 - val_loss: 52.8845\n",
      "Epoch 164/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.1587 - val_loss: 52.7181\n",
      "Epoch 165/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.1415 - val_loss: 52.3875\n",
      "Epoch 166/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 8.0926 - val_loss: 52.2169\n",
      "Epoch 167/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.0982 - val_loss: 52.0759\n",
      "Epoch 168/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.0958 - val_loss: 51.7655\n",
      "Epoch 169/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.0620 - val_loss: 51.5880\n",
      "Epoch 170/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.0710 - val_loss: 51.4775\n",
      "Epoch 171/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.0909 - val_loss: 51.2050\n",
      "Epoch 172/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.0529 - val_loss: 51.0303\n",
      "Epoch 173/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 8.0482 - val_loss: 50.9039\n",
      "Epoch 174/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.0529 - val_loss: 50.6124\n",
      "Epoch 175/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.0043 - val_loss: 50.4494\n",
      "Epoch 176/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 8.0051 - val_loss: 50.3565\n",
      "Epoch 177/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.0199 - val_loss: 50.0861\n",
      "Epoch 178/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.9643 - val_loss: 49.9029\n",
      "Epoch 179/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.9709 - val_loss: 49.8077\n",
      "Epoch 180/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.9695 - val_loss: 49.6135\n",
      "Epoch 181/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 8.0198 - val_loss: 49.4636\n",
      "Epoch 182/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.9166 - val_loss: 49.1927\n",
      "Epoch 183/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.9237 - val_loss: 49.1690\n",
      "Epoch 184/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.9633 - val_loss: 48.9188\n",
      "Epoch 185/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.9001 - val_loss: 48.7018\n",
      "Epoch 186/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.9177 - val_loss: 48.6514\n",
      "Epoch 187/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.9249 - val_loss: 48.3742\n",
      "Epoch 188/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.8843 - val_loss: 48.2337\n",
      "Epoch 189/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.9002 - val_loss: 48.1184\n",
      "Epoch 190/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.9120 - val_loss: 47.8535\n",
      "Epoch 191/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.8571 - val_loss: 47.7368\n",
      "Epoch 192/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.8535 - val_loss: 47.6546\n",
      "Epoch 193/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.8737 - val_loss: 47.4223\n",
      "Epoch 194/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.8258 - val_loss: 47.2920\n",
      "Epoch 195/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.8430 - val_loss: 47.2027\n",
      "Epoch 196/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.8630 - val_loss: 46.9725\n",
      "Epoch 197/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.8095 - val_loss: 46.8729\n",
      "Epoch 198/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.8133 - val_loss: 46.7973\n",
      "Epoch 199/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.8046 - val_loss: 46.5646\n",
      "Epoch 200/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.7847 - val_loss: 46.4586\n",
      "Epoch 201/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.8074 - val_loss: 46.2817\n",
      "Epoch 202/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.7425 - val_loss: 46.0112\n",
      "Epoch 203/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.7823 - val_loss: 46.0631\n",
      "Epoch 204/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.7976 - val_loss: 45.7698\n",
      "Epoch 205/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.7514 - val_loss: 45.6667\n",
      "Epoch 206/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.7854 - val_loss: 45.5433\n",
      "Epoch 207/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.7057 - val_loss: 45.3051\n",
      "Epoch 208/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.7461 - val_loss: 45.3316\n",
      "Epoch 209/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6804 - val_loss: 45.0204\n",
      "Epoch 210/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.7072 - val_loss: 45.0906\n",
      "Epoch 211/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.7815 - val_loss: 44.8206\n",
      "Epoch 212/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.7050 - val_loss: 44.6654\n",
      "Epoch 213/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.7383 - val_loss: 44.6187\n",
      "Epoch 214/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6560 - val_loss: 44.3564\n",
      "Epoch 215/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.6932 - val_loss: 44.3291\n",
      "Epoch 216/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6387 - val_loss: 44.0619\n",
      "Epoch 217/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.6766 - val_loss: 44.0342\n",
      "Epoch 218/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6348 - val_loss: 43.7976\n",
      "Epoch 219/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.6528 - val_loss: 43.8165\n",
      "Epoch 220/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6892 - val_loss: 43.5499\n",
      "Epoch 221/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6447 - val_loss: 43.4556\n",
      "Epoch 222/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6734 - val_loss: 43.3769\n",
      "Epoch 223/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6072 - val_loss: 43.1174\n",
      "Epoch 224/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.6471 - val_loss: 43.1191\n",
      "Epoch 225/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5956 - val_loss: 42.8927\n",
      "Epoch 226/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6289 - val_loss: 42.8621\n",
      "Epoch 227/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5849 - val_loss: 42.6096\n",
      "Epoch 228/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6084 - val_loss: 42.5337\n",
      "Epoch 229/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5755 - val_loss: 42.3657\n",
      "Epoch 230/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.6060 - val_loss: 42.2972\n",
      "Epoch 231/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5615 - val_loss: 42.0362\n",
      "Epoch 232/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6003 - val_loss: 41.9983\n",
      "Epoch 233/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5574 - val_loss: 41.8162\n",
      "Epoch 234/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5876 - val_loss: 41.8032\n",
      "Epoch 235/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5406 - val_loss: 41.5719\n",
      "Epoch 236/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.5750 - val_loss: 41.6175\n",
      "Epoch 237/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.6194 - val_loss: 41.3890\n",
      "Epoch 238/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5548 - val_loss: 41.2910\n",
      "Epoch 239/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5825 - val_loss: 41.2155\n",
      "Epoch 240/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5238 - val_loss: 41.0077\n",
      "Epoch 241/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.5678 - val_loss: 41.0117\n",
      "Epoch 242/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5146 - val_loss: 40.7545\n",
      "Epoch 243/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5373 - val_loss: 40.7261\n",
      "Epoch 244/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5075 - val_loss: 40.5426\n",
      "Epoch 245/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5385 - val_loss: 40.5222\n",
      "Epoch 246/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4997 - val_loss: 40.3342\n",
      "Epoch 247/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5326 - val_loss: 40.2578\n",
      "Epoch 248/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4913 - val_loss: 40.0400\n",
      "Epoch 249/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5267 - val_loss: 39.9987\n",
      "Epoch 250/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4760 - val_loss: 39.8063\n",
      "Epoch 251/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5085 - val_loss: 39.7667\n",
      "Epoch 252/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.4729 - val_loss: 39.5785\n",
      "Epoch 253/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5085 - val_loss: 39.5635\n",
      "Epoch 254/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4627 - val_loss: 39.3717\n",
      "Epoch 255/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5024 - val_loss: 39.3507\n",
      "Epoch 256/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 7.4594 - val_loss: 39.1719\n",
      "Epoch 257/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.4796 - val_loss: 39.1443\n",
      "Epoch 258/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4439 - val_loss: 38.9305\n",
      "Epoch 259/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4844 - val_loss: 38.8963\n",
      "Epoch 260/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4453 - val_loss: 38.7424\n",
      "Epoch 261/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4747 - val_loss: 38.7144\n",
      "Epoch 262/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4281 - val_loss: 38.4890\n",
      "Epoch 263/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4683 - val_loss: 38.4206\n",
      "Epoch 264/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4270 - val_loss: 38.2775\n",
      "Epoch 265/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.4495 - val_loss: 38.3849\n",
      "Epoch 266/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.5081 - val_loss: 38.1443\n",
      "Epoch 267/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4396 - val_loss: 37.9785\n",
      "Epoch 268/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.4741 - val_loss: 37.9155\n",
      "Epoch 269/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4075 - val_loss: 37.7781\n",
      "Epoch 270/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.4652 - val_loss: 37.8843\n",
      "Epoch 271/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.3941 - val_loss: 37.5815\n",
      "Epoch 272/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4333 - val_loss: 37.4895\n",
      "Epoch 273/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.3937 - val_loss: 37.2630\n",
      "Epoch 274/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.4269 - val_loss: 37.2665\n",
      "Epoch 275/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.3859 - val_loss: 37.1206\n",
      "Epoch 276/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.4215 - val_loss: 37.0651\n",
      "Epoch 277/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.3812 - val_loss: 36.8837\n",
      "Epoch 278/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.4236 - val_loss: 36.8646\n",
      "Epoch 279/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.3712 - val_loss: 36.6603\n",
      "Epoch 280/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.4126 - val_loss: 36.6520\n",
      "Epoch 281/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.3705 - val_loss: 36.5047\n",
      "Epoch 282/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.4105 - val_loss: 36.4861\n",
      "Epoch 283/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.3622 - val_loss: 36.2845\n",
      "Epoch 284/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.4058 - val_loss: 36.2577\n",
      "Epoch 285/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.3552 - val_loss: 36.0779\n",
      "Epoch 286/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.3950 - val_loss: 36.0567\n",
      "Epoch 287/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.3544 - val_loss: 35.9440\n",
      "Epoch 288/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.3918 - val_loss: 35.9552\n",
      "Epoch 289/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.3438 - val_loss: 35.7651\n",
      "Epoch 290/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.3906 - val_loss: 35.7583\n",
      "Epoch 291/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.3472 - val_loss: 35.6390\n",
      "Epoch 292/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.3849 - val_loss: 35.6582\n",
      "Epoch 293/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.3351 - val_loss: 35.4692\n",
      "Epoch 294/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.3808 - val_loss: 35.4396\n",
      "Epoch 295/300\n",
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.3313 - val_loss: 35.2672\n",
      "Epoch 296/300\n",
      "10785/10785 [==============================] - 0s 12us/step - loss: 7.3685 - val_loss: 35.2835\n",
      "Epoch 297/300\n",
      "10785/10785 [==============================] - 0s 15us/step - loss: 7.3263 - val_loss: 35.1180\n",
      "Epoch 298/300\n",
      "10785/10785 [==============================] - 0s 14us/step - loss: 7.3819 - val_loss: 35.1203\n",
      "Epoch 299/300\n",
      "10785/10785 [==============================] - 0s 17us/step - loss: 7.3230 - val_loss: 34.8518\n",
      "Epoch 300/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10785/10785 [==============================] - 0s 16us/step - loss: 7.3676 - val_loss: 34.8392\n"
     ]
    }
   ],
   "source": [
    "callback = ModelCheckpoint(filepath='/home/chatta/logic_rules/model_checkpoints/concept/final_weights/n100_1.h5',monitor='val_loss',save_best_only=True)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "hist=model.fit({'input_data':final_in_train,'input_pred':final_p_train},final_lbl_train,validation_data=[[final_in_val,final_p_val],final_lbl_val],callbacks=[callback],batch_size=5*512,shuffle=False, epochs=300,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------correponding weights are---------------\n",
    "# for case 1=100_1, case= 2=50_1, case:3 =10_1 case4: n10_1\n",
    "\n",
    "model=load_model(path+'logic_rules/model_checkpoints/concept/final_weights/100_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.380929522254613"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=model.predict({'input_data':final_in_test,'input_pred':final_p_test})*denom_test+min_test\n",
    "\n",
    "\n",
    "mse(preds,data[-n_test:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/chatta/logic_rules/model_checkpoints/results/simple100.txt',preds, fmt='%1.5f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/chatta/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "seed_value= 1111\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "# from sklearn.metrics import mean_absolute_percentage_error\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input,LSTM, Dense, Flatten, Conv1D, Lambda, Reshape, RepeatVector\n",
    "from keras.layers.merge import concatenate, multiply,add\n",
    "from keras import regularizers\n",
    "from keras.initializers import glorot_uniform\n",
    "from tqdm import tqdm\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import random as python_random\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to make input window for training and test sets\n",
    "\n",
    "def make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]\n",
    "    y = np.zeros([length-window_size+1-horizon,horizon])\n",
    "    output=np.zeros([length-window_size+1-horizon,window_size])\n",
    "    for i in range(length-window_size-horizon+1):\n",
    "        output[i:i+1,:]=data[i:i+window_size]\n",
    "        y[i,:]= data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],window_size), y\n",
    "\n",
    "def make_k_input(data,horizon):\n",
    "    length = data.shape[0]\n",
    "    output= np.zeros([length+1-horizon,horizon])\n",
    "    for i in range(length-horizon+1):\n",
    "        output[i:i+1,:]=data[i:i+horizon]\n",
    "    return output.reshape(output.shape[0],horizon)\n",
    "\n",
    "def nonov_make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "\n",
    "    data = np.append(data,np.zeros([horizon-extra]))\n",
    "\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "        \n",
    "    output=np.zeros([i_val,window_size])\n",
    "    y=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[i*horizon:(i*horizon)+window_size]\n",
    "        y[i,:]= data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "        \n",
    "    return output.reshape(output.shape[0],window_size), y\n",
    "\n",
    "def nonov_make_k_input(data,horizon):\n",
    "    length = data.shape[0]\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "    data_app = np.repeat(data[-1],(horizon-extra))\n",
    "    data = np.append(data,data_app)    \n",
    "\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "    output=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[(i*horizon):(i*horizon)+horizon]\n",
    "    return output.reshape(output.shape[0],horizon)\n",
    "\n",
    "\n",
    "def metrics(pred,gt):\n",
    "    l = pred.shape[1]\n",
    "#     print(l)\n",
    "    err_mse = np.zeros((l))\n",
    "    err_mae = np.zeros((l))\n",
    "\n",
    "    for i in range(l):\n",
    "        err_mse[i] = mse(pred[:,i],gt[:,i])\n",
    "        err_mae[i] = mae(pred[:,i],gt[:,i])\n",
    "        \n",
    "    return np.sqrt(np.mean(err_mse)),np.mean(err_mae)\n",
    "\n",
    "def concat(data):\n",
    "    temp=np.zeros((data.shape[0]*207,data.shape[1]))\n",
    "    for i in range(207):\n",
    "        temp[i*data.shape[0]:(i+1)*data.shape[0],:]=data[:,:,i]\n",
    "    return temp\n",
    "\n",
    "def window_normalize(data_x,data_y):\n",
    "    \n",
    "    min_in = data_x.min(1).reshape(data_x.shape[0],1)\n",
    "    max_in = data_x.max(1).reshape(data_x.shape[0],1)\n",
    "    denom = (max_in-min_in)\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size >0:\n",
    "        denom[a]=1\n",
    "    out = (data_x-min_in)/denom\n",
    "    out=out.reshape(out.shape[0],out.shape[1])\n",
    "    out_y=(data_y-min_in)/denom\n",
    "    return out,out_y,denom,min_in\n",
    "\n",
    "def std_window_normalize(data_x,data_y):\n",
    "    \n",
    "    mean_in = np.mean(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    std_in = np.std(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    \n",
    "    a = np.where(std_in == 0)[0]\n",
    "    std_in[a] = 1\n",
    "    \n",
    "    out = (data_x-mean_in)/std_in\n",
    "    out=out.reshape(out.shape[0],out.shape[1])\n",
    "    out_y=(data_y-mean_in)/std_in\n",
    "    return out,out_y,std_in,mean_in\n",
    "\n",
    "def p_window_normalize(data_x):\n",
    "    \n",
    "    min_in = data_x.min(1).reshape(data_x.shape[0],1)\n",
    "    max_in = data_x.max(1).reshape(data_x.shape[0],1)\n",
    "    denom = (max_in-min_in)\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size >0:\n",
    "        denom[a]=1\n",
    "    out = (data_x-min_in)/denom\n",
    "    out=out.reshape(out.shape[0],out.shape[1])\n",
    "    \n",
    "    return out\n",
    "\n",
    "def p_std_window_normalize(data_x):\n",
    "    \n",
    "    mean_in = np.mean(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    std_in = np.std(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    \n",
    "    a = np.where(std_in == 0)[0]\n",
    "    std_in[a] = 1\n",
    "    \n",
    "    out = (data_x-mean_in)/std_in\n",
    " \n",
    "    out=out.reshape(out.shape[0],out.shape[1])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"nasdaq\"\n",
    "horizon=6\n",
    "\n",
    "d = \"/home/chatta/logic_rules/knowledge_preds/\"\n",
    "data_path=\"/home/chatta/logic_rules/data/\"+dataset\n",
    "knowledge_pred_path =d+dataset+'/horizon_'+str(horizon)+'/t_preds.csv'\n",
    "# knowledge_preds = knowledge_preds[:12660,:]\n",
    "# data1=data\n",
    "# data = pd.read_csv('/home/chatta/logic_rules/data/traffic_30_with_std.csv',usecols=['avg_flow','std'])\n",
    "# std_data=np.asarray(data['std'])\n",
    "# data= np.asarray(data['avg_flow'])\n",
    "# data1=data[47:]\n",
    "# horizon = 1\n",
    "# window_size=3\n",
    "# knowledge_preds=np.asarray(pd.read_csv(\"/home/chatta/logic_rules/results/pacf_preds/traffic_30_std/h1/preds.csv\",header=None))\n",
    "# n_train=int(0.8*data.size)+1\n",
    "# n_test=int(.1*data.size)\n",
    "# n_val=2*n_test\n",
    "\n",
    "if dataset==\"traffic\":\n",
    "    window_size = 12   \n",
    "    data = np.asarray(pd.read_csv(data_path+\".csv\",header=None))\n",
    "    knowledge_preds = np.asarray(pd.read_csv(knowledge_pred_path,header=None))\n",
    "    n_val=2880         # index from where validation set starts\n",
    "    n_test=1440        # index from where test set starts\n",
    "    data_length = data.shape[1]\n",
    "    t_size=data.shape[0]\n",
    "    output = np.zeros((n_test,data_length))\n",
    "    final_in_train = np.zeros([1,window_size+horizon,1])\n",
    "    final_in_val =final_in_train\n",
    "    final_in_test=final_in_train\n",
    "    final_lbl_train = np.zeros([1,horizon])\n",
    "    final_lbl_val = final_lbl_train\n",
    "    final_lbl_test = final_lbl_train\n",
    "    final_p_train = final_lbl_train\n",
    "    final_p_val= final_p_train\n",
    "    final_p_test = final_p_train\n",
    "elif dataset==\"nasdaq\":\n",
    "    window_size = 180\n",
    "    data = np.asarray(pd.read_csv(data_path+\".csv\"))\n",
    "    knowledge_preds = np.asarray(pd.read_csv(knowledge_pred_path,header=None))\n",
    "    n_val = 4056       # index from where validation set starts\n",
    "    n_test = 2028      # index from where validation set starts\n",
    "    data_length = data.shape[1]\n",
    "    t_size=data.shape[0]\n",
    "    output = np.zeros((n_test,data_length))\n",
    "    final_in_train = np.zeros([1,window_size+horizon,1])\n",
    "    final_in_val =final_in_train\n",
    "    final_in_test=final_in_train\n",
    "    final_lbl_train = np.zeros([1,horizon])\n",
    "    final_lbl_val = final_lbl_train\n",
    "    final_lbl_test = final_lbl_train\n",
    "    final_p_train = final_lbl_train\n",
    "    final_p_val= final_p_train\n",
    "    final_p_test = final_p_train\n",
    "    \n",
    "else:\n",
    "    window_size = 144    \n",
    "    data = np.asarray(pd.read_csv(data_path+\".txt\",header=None))\n",
    "    knowledge_preds = np.asarray(pd.read_csv(knowledge_pred_path,header=None))\n",
    "    n_val = 3947       # index from where validation set starts\n",
    "    n_test = 1973      # index from where validation set starts\n",
    "    data_length = data.shape[1]\n",
    "    t_size=data.shape[0]\n",
    "    output = np.zeros((n_test,data_length))\n",
    "    final_in_train = np.zeros([1,window_size+horizon,1])\n",
    "    final_in_val =final_in_train\n",
    "    final_in_test=final_in_train\n",
    "    final_lbl_train = np.zeros([1,horizon])\n",
    "    final_lbl_val = final_lbl_train\n",
    "    final_lbl_test = final_lbl_train\n",
    "    final_p_train = final_lbl_train\n",
    "    final_p_val= final_p_train\n",
    "    final_p_test = final_p_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:23<00:00,  2.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# n_val=2880\n",
    "# n_test=1440\n",
    "# data_length = data.shape[1]\n",
    "# t_size=data.shape[0]\n",
    "# output = np.zeros((n_test,data_length))\n",
    "# final_in_train = np.zeros([1,window_size+horizon,1])\n",
    "# final_in_train = np.zeros([1,window_size+horizon])\n",
    "\n",
    "# final_in_val =final_in_train\n",
    "# final_in_test=final_in_train\n",
    "# final_lbl_train = np.zeros([1,horizon])\n",
    "# final_lbl_val = final_lbl_train\n",
    "# final_lbl_test = final_lbl_train\n",
    "# final_p_train = final_lbl_train\n",
    "# final_p_val= final_p_train\n",
    "# final_p_test = final_p_train\n",
    "\n",
    "#------------------for sota comparison-------------------\n",
    "if dataset=='traffic':\n",
    "\n",
    "    with tqdm(total=data_length) as pbar:\n",
    "        for i in range(data_length):\n",
    "            current_row= data[:,i]\n",
    "\n",
    "            train = current_row[:-n_val]\n",
    "            val = current_row[-(n_val+window_size):-n_test]\n",
    "            test = current_row[-(n_test+window_size):]\n",
    "\n",
    "\n",
    "    #         n_reduced=int(0.5*train.size)\n",
    "    #         train=train[n_reduced:]\n",
    "            train_sequence = make_input(train, window_size,horizon)\n",
    "            val_sequence = make_input(val,window_size,horizon)\n",
    "            test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "            if dataset !='traffic':\n",
    "\n",
    "                current_pred= knowledge_preds[:(t_size-window_size),i]\n",
    "            else:\n",
    "                current_pred= knowledge_preds[:,i]\n",
    "            train_p = current_pred[:-n_val] \n",
    "    #         train_p = current_pred[(n_reduced):-n_val]  \n",
    "            val_p = current_pred[-n_val:-n_test]\n",
    "            test_p = current_pred[-n_test:]\n",
    "            train_pred = make_k_input(train_p,horizon)\n",
    "            val_pred = make_k_input(val_p,horizon)\n",
    "            test_pred = nonov_make_k_input(test_p,horizon)\n",
    "\n",
    "            train_x=np.append(train_sequence[0],train_pred,axis=1)\n",
    "            val_x=np.append(val_sequence[0],val_pred,axis=1)\n",
    "            test_x=np.append(test_sequence[0],test_pred,axis=1)\n",
    "\n",
    "\n",
    "            final_in_train =np.append(final_in_train,train_x,axis=0)\n",
    "            final_lbl_train = np.append(final_lbl_train,train_sequence[1],axis=0)\n",
    "\n",
    "\n",
    "            final_in_val =np.append(final_in_val,val_x,axis=0)\n",
    "            final_lbl_val = np.append(final_lbl_val,val_sequence[1],axis=0)\n",
    "\n",
    "\n",
    "            final_in_test =np.append(final_in_test,test_x,axis=0)\n",
    "            final_lbl_test = np.append(final_lbl_test,test_sequence[1],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            final_p_train =np.append(final_p_train,train_pred,axis=0)\n",
    "\n",
    "\n",
    "            final_p_val =np.append(final_p_val,val_pred,axis=0)\n",
    "\n",
    "\n",
    "            final_p_test =np.append(final_p_test,test_pred,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            pbar.update(1)\n",
    "# #----------------------------with normalization----------------------------------------------------        \n",
    "        \n",
    "else:\n",
    "    with tqdm(total=data_length) as pbar:\n",
    "    for i in range(data_length):\n",
    "        current_row= data[:,i]\n",
    "\n",
    "        train = current_row[:-n_val]\n",
    "        n_reduced=int(0.5*train.size)\n",
    "        train=train[n_reduced:]\n",
    "        val = current_row[-(n_val+window_size):-n_test]\n",
    "        test = current_row[-(n_test+window_size):]\n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        temp_train_x=train_sequence[0]    \n",
    "        min_in_train = temp_train_x.min(1).reshape(temp_train_x.shape[0],1)\n",
    "        max_in_train= temp_train_x.max(1).reshape(temp_train_x.shape[0],1)\n",
    "        denom_train = (max_in_train-min_in_train)\n",
    "        a = np.where(denom_train == 0)[0]\n",
    "        denom_train[a] = max_in_train[a] \n",
    "        a = np.where(denom_train == 0)[0]\n",
    "        if a.size >0:\n",
    "            denom_train[a]=1\n",
    "        temp_train_x = (temp_train_x-min_in_train)/denom_train\n",
    "        temp_train_x=temp_train_x.reshape(temp_train_x.shape[0],temp_train_x.shape[1],1)\n",
    "        temp_train_y=(train_sequence[1]-min_in_train)/denom_train\n",
    "\n",
    "\n",
    "        temp_val_x=val_sequence[0]    \n",
    "        min_in_val = temp_val_x.min(1).reshape(temp_val_x.shape[0],1)\n",
    "        max_in_val= temp_val_x.max(1).reshape(temp_val_x.shape[0],1)\n",
    "        denom_val = (max_in_val-min_in_val)\n",
    "        a = np.where(denom_val == 0)[0]\n",
    "        denom_val[a] = max_in_val[a] \n",
    "        a = np.where(denom_val == 0)[0]\n",
    "        if a.size >0:\n",
    "            denom_val[a]=1\n",
    "        temp_val_x = (temp_val_x-min_in_val)/denom_val\n",
    "        temp_val_x=temp_val_x.reshape(temp_val_x.shape[0],temp_val_x.shape[1],1)\n",
    "        temp_val_y=(val_sequence[1]-min_in_val)/denom_val\n",
    "\n",
    "\n",
    "        temp_test_x=test_sequence[0]    \n",
    "        min_in_test = temp_test_x.min(1).reshape(temp_test_x.shape[0],1)\n",
    "        max_in_test= temp_test_x.max(1).reshape(temp_test_x.shape[0],1)\n",
    "        denom_test = (max_in_test-min_in_test)\n",
    "        a = np.where(denom_test == 0)[0]\n",
    "        denom_test[a] = max_in_test[a] \n",
    "        a = np.where(denom_test == 0)[0]\n",
    "        if a.size >0:\n",
    "            denom_test[a]=1\n",
    "        temp_test_x = (temp_test_x-min_in_test)/denom_test\n",
    "        temp_test_x=temp_test_x.reshape(temp_test_x.shape[0],temp_test_x.shape[1],1)\n",
    "        temp_test_y=(test_sequence[1]-min_in_test)/denom_test\n",
    "\n",
    "\n",
    "\n",
    "        current_pred= knowledge_preds[:(t_size-window_size),i]\n",
    "        current_pred= knowledge_preds[n_reduced:(t_size-window_size),i]\n",
    "        train_p = current_pred[:-n_val]                                        \n",
    "        val_p = current_pred[-n_val:-n_test]\n",
    "        test_p = current_pred[-n_test:]\n",
    "        train_pred = make_k_input(train_p,horizon)\n",
    "        val_pred = make_k_input(val_p,horizon)\n",
    "        test_pred = nonov_make_k_input(test_p,horizon)\n",
    "\n",
    "        temp_train_p_x=train_pred\n",
    "        min_in = temp_train_p_x.min(1).reshape(temp_train_p_x.shape[0],1)\n",
    "        max_in = temp_train_p_x.max(1).reshape(temp_train_p_x.shape[0],1)\n",
    "        denom = (max_in-min_in)\n",
    "        a = np.where(denom == 0)[0]\n",
    "        denom[a] = max_in[a] #--------------------------------------check\n",
    "        a = np.where(denom == 0)[0]\n",
    "        if len(a)>0:\n",
    "            denom[a]=1\n",
    "        temp_train_p_x = (temp_train_p_x-min_in_train)/denom_train \n",
    "#         temp_train_p_x[a] =0.5\n",
    "\n",
    "        final_p_train =np.append(final_p_train,temp_train_p_x,axis=0)\n",
    "\n",
    "        temp_val_p_x=val_pred\n",
    "        min_in = temp_val_p_x.min(1).reshape(temp_val_p_x.shape[0],1)\n",
    "        max_in = temp_val_p_x.max(1).reshape(temp_val_p_x.shape[0],1)\n",
    "        denom = (max_in-min_in)\n",
    "        a = np.where(denom == 0)[0]\n",
    "        denom[a] = max_in[a] #--------------------------------------check\n",
    "        a = np.where(denom == 0)[0]\n",
    "        if len(a)>0:\n",
    "            denom[a]=1\n",
    "        temp_val_p_x = (temp_val_p_x-min_in_val)/denom_val\n",
    "        temp_val_p_x[a] = 0.5\n",
    "\n",
    "        final_p_val =np.append(final_p_val,temp_val_p_x,axis=0)\n",
    "\n",
    "        temp_test_p_x=test_pred\n",
    "        min_in = temp_test_p_x.min(1).reshape(temp_test_p_x.shape[0],1)\n",
    "        max_in = temp_test_p_x.max(1).reshape(temp_test_p_x.shape[0],1)\n",
    "        denom = (max_in-min_in)\n",
    "        a = np.where(denom == 0)[0]\n",
    "        denom[a] = max_in[a] #--------------------------------------check\n",
    "        a = np.where(denom == 0)[0]\n",
    "        if len(a)>0:\n",
    "            denom[a]=1\n",
    "        temp_test_p_x = (temp_test_p_x-min_in_test)/denom_test\n",
    "        temp_test_p_x[a] = 0.5\n",
    "\n",
    "\n",
    "\n",
    "        final_p_test =np.append(final_p_test,temp_test_p_x,axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        temp_train_p_x =temp_train_p_x.reshape(temp_train_p_x.shape[0],temp_train_p_x.shape[1],1)\n",
    "        temp_train_x=np.append(temp_train_x,temp_train_p_x,axis=1)\n",
    "        final_in_train =np.append(final_in_train,temp_train_x,axis=0)\n",
    "        final_lbl_train = np.append(final_lbl_train,temp_train_y,axis=0)\n",
    "        \n",
    "        temp_val_p_x =temp_val_p_x.reshape(temp_val_p_x.shape[0],temp_val_p_x.shape[1],1)\n",
    "        temp_val_x=np.append(temp_val_x,temp_val_p_x,axis=1)\n",
    "        final_in_val =np.append(final_in_val,temp_val_x,axis=0)\n",
    "        final_lbl_val = np.append(final_lbl_val,temp_val_y,axis=0)\n",
    "        \n",
    "        temp_test_p_x =temp_test_p_x.reshape(temp_test_p_x.shape[0],temp_test_p_x.shape[1],1)\n",
    "        temp_test_x=np.append(temp_test_x,temp_test_p_x,axis=1)\n",
    "        final_in_test =np.append(final_in_test,temp_test_x,axis=0)\n",
    "        final_lbl_test = np.append(final_lbl_test,temp_test_y,axis=0)\n",
    "\n",
    "        pbar.update(1)\n",
    "final_in_train = final_in_train[1:,:,:]\n",
    "final_in_val = final_in_val[1:,:,:]\n",
    "final_in_test = final_in_test[1:,:,:]\n",
    "final_lbl_train = final_lbl_train[1:,:]\n",
    "final_lbl_val = final_lbl_val[1:,:]\n",
    "final_lbl_test = final_lbl_test[1:,:]\n",
    "final_p_train = final_p_train[1:,:]\n",
    "final_p_val = final_p_val[1:,:]\n",
    "final_p_test = final_p_test[1:,:]\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406458, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_p_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406458, 156, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_in_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "K.clear_session()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_data= Input(batch_shape=(None,window_size+horizon,1),name='input_data')\n",
    "input_pred=Input(batch_shape=(None,horizon),name='input_pred')\n",
    "\n",
    "if dataset!='traffic':\n",
    "    branch_0 = Conv1D(4,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform())(input_data)\n",
    "    branch_1 = Conv1D(32,3, strides=1,dilation_rate=1,padding='same',activation='relu',kernel_initializer=glorot_uniform())(branch_0)\n",
    "    branch_2 = Conv1D(64,3, strides=1,dilation_rate=1, padding='same',activation='relu',kernel_initializer=glorot_uniform())(branch_1)\n",
    "\n",
    "    branch_4=Flatten()(branch_2)\n",
    "    net= Dense(128,activation='relu')(branch_4)\n",
    "    net= Dense(horizon,activation='relu')(net)\n",
    "    net=add([net,input_pred])\n",
    "#         ,activity_regularizer=regularizers.l2(0.01)\n",
    "else:\n",
    "    branch_0 = LSTM(512,activation='sigmoid',kernel_initializer=glorot_uniform(1),return_sequences=True,name='lstm_1')(input_data)\n",
    "    branch_1 = LSTM(64,activation='relu',kernel_initializer=glorot_uniform(2),return_sequences=True,name='lstm_2')(branch_0)\n",
    "    branch_2 = LSTM(16,activation='relu',kernel_initializer=glorot_uniform(3),name='lstm_3')(branch_1)\n",
    "    net= Dense(horizon,name='dense_final2')(branch_2)\n",
    "    net=add([net,input_pred])\n",
    "\n",
    "model=Model(inputs=[input_data,input_pred],outputs=net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1481494 samples, validate on 165886 samples\n",
      "Epoch 1/50\n",
      "1481494/1481494 [==============================] - 14s 10us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 2/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 3/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 4/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 5/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 6/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 7/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 8/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 9/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 10/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 11/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 12/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 13/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 14/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 15/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 16/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 17/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 18/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 19/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 20/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 21/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 22/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 23/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 24/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 25/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 26/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 27/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 28/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 29/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 30/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 31/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 32/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 33/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 34/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 35/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 36/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 37/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 38/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 39/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 40/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 41/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 42/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 43/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 44/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 45/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 46/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 47/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 48/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 49/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n",
      "Epoch 50/50\n",
      "1481494/1481494 [==============================] - 14s 9us/step - loss: 0.0305 - val_loss: 0.0233\n"
     ]
    }
   ],
   "source": [
    "callback = ModelCheckpoint(filepath='/home/chatta/logic_rules/model_checkpoints/'+dataset+'/h'+str(horizon)+'/dnn_50_1.h5',monitor='val_loss',save_best_only=True)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "hist=model.fit({'input_data':final_in_train,'input_pred':final_p_train},final_lbl_train,validation_data=[[final_in_val,final_p_val],final_lbl_val],callbacks=[callback],batch_size=4*512,shuffle=False, epochs=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('/home/chatta/logic_rules/model_checkpoints/'+dataset+'/h'+str(horizon)+'/dnn_50_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data_length):\n",
    "    \n",
    "    if dataset==\"traffic\":\n",
    "        current_row= data[:,i]\n",
    "\n",
    "        test = current_row[-(n_test+window_size):]\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        \n",
    "\n",
    "        current_pred= knowledge_preds[:,i]\n",
    "        test_p = current_pred[-n_test:]\n",
    "        test_pred = nonov_make_k_input(test_p,horizon)\n",
    "        norm_p_test_x=p_window_normalize(test_pred) \n",
    "\n",
    "        test_x=np.append(test_sequence[0],test_pred,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pred = model.predict({'input_data':test_x.reshape(test_x.shape[0],test_x.shape[1],1),'input_pred':test_pred})\n",
    "    #     pred=pred*denom+min_in\n",
    "        prediction = pred.flatten()[:n_test]\n",
    "\n",
    "        output[:,i]=np.transpose(prediction)\n",
    "    else:\n",
    "        current_row= data[:,i]\n",
    "        series_d = current_row\n",
    "        test = series_d[-(n_test+window_size):]\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        temp_test_x=test_sequence[0]    \n",
    "\n",
    "        min_in_test = temp_test_x.min(1).reshape(temp_test_x.shape[0],1)\n",
    "        max_in_test = temp_test_x.max(1).reshape(temp_test_x.shape[0],1)\n",
    "        denom_test = (max_in_test-min_in_test)\n",
    "        a = np.where(denom_test == 0)[0]\n",
    "        denom_test[a] = max_in_test[a]  #----------------------------------check\n",
    "        temp_test_x = (temp_test_x-min_in_test)/denom_test\n",
    "        temp_test_x=temp_test_x.reshape(temp_test_x.shape[0],temp_test_x.shape[1],1)\n",
    "        temp_test_y=(test_sequence[1]-min_in_test)/denom_test\n",
    "\n",
    "        current_pred= knowledge_preds[:,i]\n",
    "        \n",
    "        test_p = current_pred[-n_test:]\n",
    "        test_k_pred = nonov_make_k_input(test_p,horizon)\n",
    "\n",
    "        temp_test_p_x=test_k_pred\n",
    "        min_in = temp_test_p_x.min(1).reshape(temp_test_p_x.shape[0],1)\n",
    "        max_in = temp_test_p_x.max(1).reshape(temp_test_p_x.shape[0],1)\n",
    "        denom = (max_in-min_in)\n",
    "        a = np.where(denom == 0)[0]\n",
    "        denom[a] = max_in[a] #--------------------------------------check\n",
    "        a = np.where(denom == 0)[0]\n",
    "        if len(a)>0:\n",
    "            denom[a]=1\n",
    "        temp_test_p_x = (temp_test_p_x-min_in_test)/denom_test\n",
    "        temp_test_p_x[a] = 0.5\n",
    "\n",
    "        temp_test_p_x1 =temp_test_p_x.reshape(temp_test_p_x.shape[0],temp_test_p_x.shape[1],1)\n",
    "        temp_test_x=np.append(temp_test_x,temp_test_p_x1,axis=1)\n",
    "#         final_in_test =np.append(final_in_test,temp_test_x,axis=0)\n",
    "#         final_lbl_test = np.append(final_lbl_test,temp_test_y,axis=0)\n",
    "\n",
    "        pred = model.predict({'input_data':temp_test_x, 'input_pred':temp_test_p_x})\n",
    "        prediction = pred*(max_in_test-min_in_test)+min_in_test\n",
    "        prediction = prediction.flatten()[:n_test]\n",
    "\n",
    "        output[:,i]=np.transpose(prediction)\n",
    "    \n",
    "# if dataset==\"concept\":\n",
    "#     # preds=model.predict({'input_data':final_in_test,'input_pred':final_p_test})*denom_test+min_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24697988199680967, 0.06291757391341506]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1, temp2= metrics(output[-n_test:,:],data[-n_test:,:])\n",
    "[temp1,temp2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('/home/chatta/logic_rules/results/pacf_preds/traffic_30_std/h1/preds_noisy.csv',knowledge_preds, fmt='%1.5f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

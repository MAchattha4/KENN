{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "seed_value= 1111\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "# from sklearn.metrics import mean_absolute_percentage_error\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input,LSTM, Dense, Flatten, Conv1D, Lambda, Reshape, RepeatVector\n",
    "from keras.layers.merge import concatenate, multiply,add\n",
    "from keras import regularizers\n",
    "from keras.initializers import glorot_uniform\n",
    "from tqdm import tqdm\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import random as python_random\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to make input window for training and test sets\n",
    "\n",
    "def make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]\n",
    "    y = np.zeros([length-window_size+1-horizon,horizon])\n",
    "    output=np.zeros([length-window_size+1-horizon,window_size])\n",
    "    for i in range(length-window_size-horizon+1):\n",
    "        output[i:i+1,:]=data[i:i+window_size]\n",
    "        y[i,:]= data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],window_size), y\n",
    "\n",
    "def make_k_input(data,horizon):\n",
    "    length = data.shape[0]\n",
    "    output= np.zeros([length+1-horizon,horizon])\n",
    "    for i in range(length-horizon+1):\n",
    "        output[i:i+1,:]=data[i:i+horizon]\n",
    "    return output.reshape(output.shape[0],horizon)\n",
    "\n",
    "def nonov_make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "\n",
    "    data = np.append(data,np.zeros([horizon-extra]))\n",
    "\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "        \n",
    "    output=np.zeros([i_val,window_size])\n",
    "    y=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[i*horizon:(i*horizon)+window_size]\n",
    "        y[i,:]= data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "        \n",
    "    return output.reshape(output.shape[0],window_size), y\n",
    "\n",
    "def nonov_make_k_input(data,horizon):\n",
    "    length = data.shape[0]\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "    data_app = np.repeat(data[-1],(horizon-extra))\n",
    "    data = np.append(data,data_app)    \n",
    "\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "    output=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[(i*horizon):(i*horizon)+horizon]\n",
    "    return output.reshape(output.shape[0],horizon)\n",
    "\n",
    "\n",
    "def metrics(pred,gt):\n",
    "    l = pred.shape[1]\n",
    "#     print(l)\n",
    "    err_mse = np.zeros((l))\n",
    "    err_mae = np.zeros((l))\n",
    "\n",
    "    for i in range(l):\n",
    "        err_mse[i] = mse(pred[:,i],gt[:,i])\n",
    "        err_mae[i] = mae(pred[:,i],gt[:,i])\n",
    "        \n",
    "    return np.mean(err_mse),np.mean(err_mae)\n",
    "\n",
    "def concat(data):\n",
    "    temp=np.zeros((data.shape[0]*207,data.shape[1]))\n",
    "    for i in range(207):\n",
    "        temp[i*data.shape[0]:(i+1)*data.shape[0],:]=data[:,:,i]\n",
    "    return temp\n",
    "\n",
    "def window_normalize(data_x,data_y):\n",
    "    \n",
    "    \n",
    "    min_in = np.min(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    max_in = np.max(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    denom = (max_in-min_in)\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size >0:\n",
    "        denom[a]=1\n",
    "    out = (data_x-min_in)/denom\n",
    "    out=out.reshape(out.shape[0],out.shape[1],1)\n",
    "    out_y=(data_y-min_in)/denom\n",
    "    \n",
    "    return out, out_y, denom, min_in\n",
    "\n",
    "def window_normalize_with_expert(data_x,data_y,expert):\n",
    "    \n",
    "    min_in = np.min(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    max_in = np.max(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    denom = (max_in-min_in)\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size >0:\n",
    "        denom[a]=1\n",
    "    out = (data_x-min_in)/denom\n",
    "    expert_normd=(expert-min_in)/denom\n",
    "    out=np.append(out,expert_normd,axis=1)\n",
    "    out=out.reshape(out.shape[0],out.shape[1],1)\n",
    "    out_y=(data_y-min_in)/denom\n",
    "    \n",
    "    return out,out_y,expert_normd,denom,min_in\n",
    "\n",
    "def std_window_normalize(data_x,data_y):\n",
    "    \n",
    "    mean_in = np.mean(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    std_in = np.std(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    \n",
    "    a = np.where(std_in == 0)[0]\n",
    "    std_in[a] = 1\n",
    "    \n",
    "    out = (data_x-mean_in)/std_in\n",
    "    out=out.reshape(out.shape[0],out.shape[1])\n",
    "    out_y=(data_y-mean_in)/std_in\n",
    "    return out,out_y,std_in,mean_in\n",
    "\n",
    "def p_window_normalize(data_x):\n",
    "    \n",
    "    min_in = np.min(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    max_in = np.max(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    denom = (max_in-min_in)\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size >0:\n",
    "        denom[a]=1\n",
    "    out = (data_x-min_in)/denom\n",
    "    out=out.reshape(out.shape[0],out.shape[1])\n",
    "    \n",
    "    return out\n",
    "\n",
    "def p_std_window_normalize(data_x):\n",
    "    \n",
    "    mean_in = np.mean(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    std_in = np.std(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    \n",
    "    a = np.where(std_in == 0)[0]\n",
    "    std_in[a] = 1\n",
    "    \n",
    "    out = (data_x-mean_in)/std_in\n",
    " \n",
    "    out=out.reshape(out.shape[0],out.shape[1])\n",
    "    \n",
    "    return out\n",
    "\n",
    "def new_metric(pred, label):\n",
    "    with np.errstate(divide = 'ignore', invalid = 'ignore'):\n",
    "        mask = np.not_equal(label, 0)\n",
    "        mask = mask.astype(np.float32)\n",
    "        mask /= np.mean(mask)\n",
    "        mae = np.abs(np.subtract(pred, label)).astype(np.float32)\n",
    "        rmse = np.square(mae)\n",
    "        mape = np.divide(mae, label)\n",
    "        mae = np.nan_to_num(mae * mask)\n",
    "        mae = np.mean(mae)\n",
    "        rmse = np.nan_to_num(rmse * mask)\n",
    "        rmse = np.sqrt(np.mean(rmse))\n",
    "        mape = np.nan_to_num(mape * mask)\n",
    "        mape = np.mean(mape)\n",
    "    return mae, rmse, mape\n",
    "\n",
    "\n",
    "def window_normalize_with_expert2(data_x,data_y,expert):\n",
    "    \n",
    "    min_in = np.min(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    max_in = np.max(data_x,axis=1).reshape(data_x.shape[0],1)\n",
    "    denom = (max_in-min_in)\n",
    "    a = np.where(denom == 0)[0]\n",
    "    denom[a] = max_in[a] \n",
    "    a = np.where(denom == 0)[0]\n",
    "    if a.size >0:\n",
    "        denom[a]=1\n",
    "    out = (data_x-min_in)/denom\n",
    "#     expert_normd=(expert-min_in)/denom\n",
    "    out=np.append(out,expert,axis=1)\n",
    "    out=out.reshape(out.shape[0],out.shape[1],1)\n",
    "#     out_y=(data_y-min_in)/denom\n",
    "    \n",
    "    return out,data_y,expert,denom,min_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----data loading -----------------\n",
    "\n",
    "\n",
    "dataset=\"illness\" # choose frm ['traffic','nasdaq','energy','illness']\n",
    "\n",
    "# for traffic horizon=[6,9], nasdaq= [6,12], energy = [6,12], illness = [24,36] \n",
    "horizon=24  \n",
    "\n",
    "d = \"knowledge_preds/\" # parent folder containing predictions from KDS\n",
    "data_path=\"/home/chatta/logic_rules/data/\"+dataset # dataset path\n",
    "knowledge_pred_path =d+dataset+'/horizon_'+str(horizon)+'/t_preds.csv'\n",
    "\n",
    "\n",
    "if dataset==\"traffic\":\n",
    "    window_size = 12   \n",
    "    data = np.asarray(pd.read_csv(data_path+\".csv\",header=None))\n",
    "    knowledge_preds = np.asarray(pd.read_csv(knowledge_pred_path,header=None))\n",
    "    n_val=2*1440        # index from where validation set starts\n",
    "    n_test=1440        # index from where test set starts\n",
    "    data_length = data.shape[1]\n",
    "    t_size=data.shape[0]\n",
    "    output = np.zeros((n_test,data_length))\n",
    "    final_in_train = np.zeros([1,window_size+horizon,1])\n",
    "    final_in_val =final_in_train\n",
    "    final_in_test=final_in_train\n",
    "    final_lbl_train = np.zeros([1,horizon])\n",
    "    final_lbl_val = final_lbl_train\n",
    "    final_lbl_test = final_lbl_train\n",
    "    final_p_train = final_lbl_train\n",
    "    final_p_val= final_p_train\n",
    "    final_p_test = final_p_train\n",
    "elif dataset==\"nasdaq\":\n",
    "    window_size = 180\n",
    "    data = np.asarray(pd.read_csv(data_path+\".csv\"))\n",
    "    knowledge_preds = np.asarray(pd.read_csv(knowledge_pred_path,header=None))\n",
    "    n_val = 4056       # index from where validation set starts\n",
    "    n_test = 2028      # index from where validation set starts\n",
    "    data_length = data.shape[1]\n",
    "    t_size=data.shape[0]\n",
    "    output = np.zeros((n_test,data_length))\n",
    "    final_in_train = np.zeros([1,window_size+horizon,1])\n",
    "    final_in_val =final_in_train\n",
    "    final_in_test=final_in_train\n",
    "    final_lbl_train = np.zeros([1,horizon])\n",
    "    final_lbl_val = final_lbl_train\n",
    "    final_lbl_test = final_lbl_train\n",
    "    final_p_train = final_lbl_train\n",
    "    final_p_val= final_p_train\n",
    "    final_p_test = final_p_train\n",
    "    \n",
    "elif dataset=='energy':\n",
    "    window_size = 144    \n",
    "    data = np.asarray(pd.read_csv(data_path+\".txt\",header=None))\n",
    "    knowledge_preds = np.asarray(pd.read_csv(knowledge_pred_path,header=None))\n",
    "    n_val = 3947       # index from where validation set starts\n",
    "    n_test = 1973      # index from where validation set starts\n",
    "    data_length = data.shape[1]\n",
    "    t_size=data.shape[0]\n",
    "    output = np.zeros((n_test,data_length))\n",
    "    final_in_train = np.zeros([1,window_size+horizon,1])\n",
    "    final_in_val =final_in_train\n",
    "    final_in_test=final_in_train\n",
    "    final_lbl_train = np.zeros([1,horizon])\n",
    "    final_lbl_val = final_lbl_train\n",
    "    final_lbl_test = final_lbl_train\n",
    "    final_p_train = final_lbl_train\n",
    "    final_p_val= final_p_train\n",
    "    final_p_test = final_p_train\n",
    "elif dataset=='illness':\n",
    "    window_size=60\n",
    "    data= np.asarray(pd.read_csv(data_path+\".csv\",usecols=[7]))\n",
    "    n_train=966-193-77\n",
    "    \n",
    "    n_val=77\n",
    "\n",
    "    n_test=193\n",
    "    \n",
    "    train_data= data[:(n_train)]\n",
    "    train_mean = np.mean(train_data)\n",
    "    train_std= np.std(train_data)\n",
    "    data=(data--train_mean)/train_std\n",
    "    train_data= (train_data-train_mean)/train_std\n",
    "    val_data=(data[-(n_test+n_val+window_size):-n_test]-train_mean)/train_std\n",
    "\n",
    "    test_data = (data[-(n_test+window_size):]-train_mean)/train_std\n",
    "    knowledge_pred_path =d+dataset+\"/ot/\"\n",
    "else:\n",
    "    window_size=96\n",
    "    data = np.asarray(pd.read_csv(data_path+\".csv\",index_col=0))\n",
    "    knowledge_pred_path=d+dataset+\"/h\"+str(horizon)+\"/\"\n",
    "    data=data[:,1:]\n",
    "    n_test=int(data.shape[0]*0.2)\n",
    "    train_data=data[:-n_test,:]\n",
    "    mean_in=np.mean(train_data,axis=0)\n",
    "    std_in =np.std(train_data,axis=0)\n",
    "    data=(data-mean_in)/std_in\n",
    "    train_data=data[:-n_test,:]\n",
    "    test_data=data[-(n_test+window_size):,:]\n",
    "    data_length = data.shape[1]\n",
    "    t_size=data.shape[0]\n",
    "    output = np.zeros((n_test,data_length))\n",
    "    final_in_train = np.zeros([1,window_size+horizon,1])\n",
    "    final_in_val =final_in_train\n",
    "    final_in_test=final_in_train\n",
    "    final_lbl_train = np.zeros([1,horizon])\n",
    "    final_lbl_val = final_lbl_train\n",
    "    final_lbl_test = final_lbl_train\n",
    "    final_p_train = final_lbl_train\n",
    "    final_p_val= final_p_train\n",
    "    final_p_test = final_p_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#------------------for sota comparison-------------------\n",
    "if dataset=='traffic':\n",
    "\n",
    "    with tqdm(total=data_length) as pbar:\n",
    "        for i in range(data_length):\n",
    "            current_row= data[:,i]\n",
    "\n",
    "            train = current_row[:-n_val]\n",
    "            val = current_row[-(n_val+window_size):-n_test]\n",
    "            test = current_row[-(n_test+window_size):]\n",
    "\n",
    "\n",
    "            train_sequence = make_input(train, window_size,horizon)\n",
    "            val_sequence = make_input(val,window_size,horizon)\n",
    "            test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "            if dataset !='traffic':\n",
    "\n",
    "                current_pred= knowledge_preds[:(t_size-window_size),i]\n",
    "            else:\n",
    "                current_pred= knowledge_preds[:,i]\n",
    "            train_p = current_pred[:-n_val] \n",
    "\n",
    "            val_p = current_pred[-n_val:-n_test]\n",
    "            test_p = current_pred[-n_test:]\n",
    "            train_pred = make_k_input(train_p,horizon)\n",
    "            val_pred = make_k_input(val_p,horizon)\n",
    "            test_pred = nonov_make_k_input(test_p,horizon)\n",
    "\n",
    "            train_x=np.append(train_sequence[0],train_pred,axis=1)\n",
    "            val_x=np.append(val_sequence[0],val_pred,axis=1)\n",
    "            test_x=np.append(test_sequence[0],test_pred,axis=1)\n",
    "            \n",
    "            train_x = train_x.reshape(-1,window_size+horizon,1)\n",
    "            val_x = val_x.reshape(-1,window_size+horizon,1)\n",
    "\n",
    "            final_in_train =np.append(final_in_train,train_x,axis=0)\n",
    "            final_lbl_train = np.append(final_lbl_train,train_sequence[1],axis=0)\n",
    "\n",
    "\n",
    "            final_in_val =np.append(final_in_val,val_x,axis=0)\n",
    "            final_lbl_val = np.append(final_lbl_val,val_sequence[1],axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            final_p_train =np.append(final_p_train,train_pred,axis=0)\n",
    "\n",
    "\n",
    "            final_p_val =np.append(final_p_val,val_pred,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pbar.update(1)\n",
    "# #----------------------------with normalization----------------------------------------------------        \n",
    "\n",
    "    \n",
    "    \n",
    "elif dataset==\"exchange\":\n",
    "    with tqdm(total=data_length) as pbar:\n",
    "        for i in range(data_length):\n",
    "            train= train_data[:,i]\n",
    "\n",
    "\n",
    "            n_reduced=int(0*train.size)\n",
    "            train=train[n_reduced:]\n",
    "\n",
    "\n",
    "            train_sequence = make_input(train, window_size,horizon)\n",
    "            temp_train_p_x=np.asarray(pd.read_csv(knowledge_pred_path+'train_'+str(i+1)+'.csv',index_col=0))\n",
    "\n",
    "            temp_train_p_x =p_window_normalize(temp_train_p_x)\n",
    "            \n",
    "            temp_train_x,temp_train_y,_,_=window_normalize(train_sequence[0],train_sequence[1])\n",
    "\n",
    " \n",
    "            \n",
    "   \n",
    "            \n",
    "            final_p_train =np.append(final_p_train,temp_train_p_x,axis=0)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "            temp_train_p_x =temp_train_p_x.reshape(temp_train_p_x.shape[0],temp_train_p_x.shape[1],1)\n",
    "            temp_train_x=np.append(temp_train_x,temp_train_p_x,axis=1)\n",
    "            final_in_train =np.append(final_in_train,temp_train_x,axis=0)\n",
    "            final_lbl_train = np.append(final_lbl_train,temp_train_y,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pbar.update(1)\n",
    "    \n",
    "elif dataset==\"nasdaq\":\n",
    "    with tqdm(total=data_length) as pbar:\n",
    "        for i in range(data_length):\n",
    "            current_row= data[:,i]\n",
    "\n",
    "            train = current_row[:-n_val]\n",
    "            n_reduced=int(0*train.size)\n",
    "            train=train[n_reduced:]\n",
    "            val = current_row[-(n_val+window_size):-n_test]\n",
    "            test = current_row[-(n_test+window_size):]\n",
    "            train_sequence = make_input(train, window_size,horizon)\n",
    "            val_sequence = make_input(val,window_size,horizon)\n",
    "            test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "            \n",
    "            current_pred= knowledge_preds[:(t_size-window_size),i]\n",
    "            current_pred= knowledge_preds[n_reduced:(t_size-window_size),i]            \n",
    "            train_p = current_pred[:-n_val]                                        \n",
    "            val_p = current_pred[-n_val:-n_test]\n",
    "            test_p = current_pred[-n_test:]\n",
    "            train_pred = make_k_input(train_p,horizon)\n",
    "            val_pred = make_k_input(val_p,horizon)\n",
    "            test_pred = nonov_make_k_input(test_p,horizon)\n",
    "            \n",
    "            temp_train_x,temp_train_y,_,_=window_normalize(train_sequence[0],train_sequence[1])\n",
    "            temp_val_x,temp_val_y,_,_=window_normalize(val_sequence[0],val_sequence[1])\n",
    "            temp_train_p_x=p_window_normalize(train_pred)\n",
    "            temp_val_p_x=p_window_normalize(val_pred)\n",
    "            \n",
    "            final_p_train =np.append(final_p_train,temp_train_p_x,axis=0)\n",
    "            final_p_val =np.append(final_p_val,temp_val_p_x,axis=0) \n",
    "\n",
    "\n",
    "\n",
    "            temp_train_p_x =temp_train_p_x.reshape(temp_train_p_x.shape[0],temp_train_p_x.shape[1],1)\n",
    "            temp_train_x=np.append(temp_train_x,temp_train_p_x,axis=1)\n",
    "            final_in_train =np.append(final_in_train,temp_train_x,axis=0)\n",
    "            final_lbl_train = np.append(final_lbl_train,temp_train_y,axis=0)\n",
    "\n",
    "            temp_val_p_x =temp_val_p_x.reshape(temp_val_p_x.shape[0],temp_val_p_x.shape[1],1)\n",
    "            temp_val_x=np.append(temp_val_x,temp_val_p_x,axis=1)\n",
    "            final_in_val =np.append(final_in_val,temp_val_x,axis=0)\n",
    "            final_lbl_val = np.append(final_lbl_val,temp_val_y,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            pbar.update(1)\n",
    "else:\n",
    "    with tqdm(total=data_length) as pbar:\n",
    "        for i in range(data_length):\n",
    "            current_row= data[:,i]\n",
    "\n",
    "            train = current_row[:-n_val]\n",
    "            n_reduced=int(0.5*train.size)\n",
    "            train=train[n_reduced:]\n",
    "            val = current_row[-(n_val+window_size):-n_test]\n",
    "            test = current_row[-(n_test+window_size):]\n",
    "            train_sequence = make_input(train, window_size,horizon)\n",
    "            val_sequence = make_input(val,window_size,horizon)\n",
    "            test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "            \n",
    "            current_pred= knowledge_preds[:(t_size-window_size),i]\n",
    "            current_pred= knowledge_preds[n_reduced:(t_size-window_size),i]            \n",
    "            train_p = current_pred[:-n_val]                                        \n",
    "            val_p = current_pred[-n_val:-n_test]\n",
    "            test_p = current_pred[-n_test:]\n",
    "            train_pred = make_k_input(train_p,horizon)\n",
    "            val_pred = make_k_input(val_p,horizon)\n",
    "            test_pred = nonov_make_k_input(test_p,horizon)\n",
    "            \n",
    "            temp_train_x,temp_train_y,temp_train_p_x,_,_= window_normalize_with_expert(train_sequence[0],train_sequence[1],train_pred)\n",
    "            temp_val_x,temp_val_y,temp_val_p_x,_,_=window_normalize_with_expert(val_sequence[0],val_sequence[1],val_pred)\n",
    "   \n",
    "            \n",
    "            final_p_train =np.append(final_p_train,temp_train_p_x,axis=0)\n",
    "            final_p_val =np.append(final_p_val,temp_val_p_x,axis=0) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            final_in_train =np.append(final_in_train,temp_train_x,axis=0)\n",
    "            final_lbl_train = np.append(final_lbl_train,temp_train_y,axis=0)\n",
    "\n",
    "            \n",
    "            final_in_val =np.append(final_in_val,temp_val_x,axis=0)\n",
    "            final_lbl_val = np.append(final_lbl_val,temp_val_y,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "            pbar.update(1)\n",
    "    \n",
    "final_in_train = final_in_train[1:,:,:]\n",
    "final_in_val = final_in_val[1:,:,:]\n",
    "final_in_test = final_in_test[1:,:,:]\n",
    "final_lbl_train = final_lbl_train[1:,:]\n",
    "final_lbl_val = final_lbl_val[1:,:]\n",
    "final_lbl_test = final_lbl_test[1:,:]\n",
    "final_p_train = final_p_train[1:,:]\n",
    "final_p_val = final_p_val[1:,:]\n",
    "final_p_test = final_p_test[1:,:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------Setting up the network----------------\n",
    "\n",
    "tf.reset_default_graph()\n",
    "K.clear_session()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_data= Input(batch_shape=(None,window_size+horizon,1),name='input_data')\n",
    "input_pred=Input(batch_shape=(None,horizon),name='input_pred')\n",
    "\n",
    "if dataset=='traffic' or dataset==\"energy\" or dataset=='exchange':\n",
    "    branch_0 = Conv1D(4,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform())(input_data)\n",
    "    branch_1 = Conv1D(32,3, strides=1,dilation_rate=1,padding='same',activation='relu',kernel_initializer=glorot_uniform())(branch_0)\n",
    "    branch_2 = Conv1D(64,3, strides=1,dilation_rate=1, padding='same',activation='relu',kernel_initializer=glorot_uniform())(branch_1)\n",
    "\n",
    "    branch_4=Flatten()(branch_2)\n",
    "    net= Dense(128,activation='relu')(branch_4)\n",
    "    net= Dense(horizon,activation='relu')(net)\n",
    "    net=add([net,input_pred])\n",
    "\n",
    "else:\n",
    "    branch_0 = LSTM(512,activation='sigmoid',kernel_initializer=glorot_uniform(1),return_sequences=True,name='lstm_1')(input_data)\n",
    "    branch_1 = LSTM(64,activation='relu',kernel_initializer=glorot_uniform(2),return_sequences=True,name='lstm_2')(branch_0)\n",
    "    branch_2 = LSTM(16,activation='relu',kernel_initializer=glorot_uniform(3),name='lstm_3')(branch_1)\n",
    "    net= Dense(horizon,name='dense_final2')(branch_2)\n",
    "    net=add([net,input_pred])\n",
    "\n",
    "model=Model(inputs=[input_data,input_pred],outputs=net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint(filepath='/home/chatta/logic_rules/model_checkpoints/'+dataset+'/h'+str(horizon)+'/temp.h5',monitor='val_loss',save_best_only=True)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001))\n",
    "hist=model.fit({'input_data':final_in_train,'input_pred':final_p_train},final_lbl_train,validation_split=0.1,callbacks=[callback],batch_size=256,shuffle=False, epochs=30,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('/home/chatta/logic_rules/model_checkpoints/'+dataset+'/h'+str(horizon)+'/kenn_best.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1=np.zeros((n_test,data_length))\n",
    "output2= np.zeros((n_test,data_length))\n",
    "for i in range(data_length):\n",
    "    \n",
    "    if dataset==\"traffic\":\n",
    "        current_row= data[:,i]\n",
    "\n",
    "        test = current_row[-(n_test+window_size):]\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        \n",
    "\n",
    "        current_pred= knowledge_preds[:,i]\n",
    "        test_p = current_pred[-n_test:]\n",
    "        test_pred = nonov_make_k_input(test_p,horizon)\n",
    "        norm_p_test_x=p_window_normalize(test_pred) \n",
    "\n",
    "        test_x=np.append(test_sequence[0],test_pred,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pred = model.predict({'input_data':test_x.reshape(test_x.shape[0],test_x.shape[1],1),'input_pred':test_pred})\n",
    "\n",
    "        prediction = pred.flatten()[:n_test]\n",
    "        \n",
    "\n",
    "        output[:,i]=np.transpose(prediction)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    elif dataset==\"exchange\":\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        temp_test_x,temp_test_y=nonov_make_input(test_data[:,i],window_size,horizon)\n",
    "        theta_test=np.nan_to_num(np.asarray(pd.read_csv(knowledge_pred_path+'test_'+str(i+1)+'.csv',index_col=0)))\n",
    "\n",
    "        output2[:,i]=theta_test.flatten()[:n_test]\n",
    "        theta_test=p_window_normalize(theta_test)\n",
    "        temp_test_x,temp_test_y,denom_test,min_in_test = window_normalize(temp_test_x,temp_test_y)\n",
    "\n",
    "        temp_test_x=np.append(temp_test_x,temp_test_p_x,axis=1)\n",
    "        pred = model.predict({'input_data':temp_test_x, 'input_pred':theta_test})\n",
    "\n",
    "        \n",
    "        prediction = pred*(denom_test)+min_in_test\n",
    "\n",
    "        prediction = prediction.flatten()[:n_test]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        output[:,i]=np.transpose(prediction)\n",
    "\n",
    "        \n",
    "\n",
    "     \n",
    "    else:\n",
    "        current_row= data[:,i]\n",
    "        series_d = current_row\n",
    "        test = series_d[-(n_test+window_size):]\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "        current_pred= knowledge_preds[:,i]        \n",
    "        test_p = current_pred[-n_test:]\n",
    "        test_k_pred = nonov_make_k_input(test_p,horizon)\n",
    "        \n",
    "        temp_test_x,_,temp_test_p_x,denom_test,min_in_test=window_normalize_with_expert(test_sequence[0],test_sequence[1],test_k_pred)\n",
    "        \n",
    "\n",
    "        pred = model.predict({'input_data':temp_test_x, 'input_pred':temp_test_p_x})\n",
    "\n",
    "        prediction = pred*(denom_test)+min_in_test\n",
    "\n",
    "        prediction = prediction.flatten()[:n_test]\n",
    "\n",
    "        output[:,i]=np.transpose(prediction)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------for illness----------\n",
    "\n",
    "\n",
    "temp1, temp2= metrics(output[-n_test:,:],data[-n_test:,:])\n",
    "\n",
    "[temp1,temp2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/chatta/logic_rules/preds_for_plots/exchange_h192.csv',output, fmt='%1.5f',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
